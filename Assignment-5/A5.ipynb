{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaustav Vats (2016048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, re\n",
    "import json\n",
    "from math import log\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "# References\n",
    "# Cosine Similarity and TFIDF Matrix\n",
    "# https://github.com/parulnith/Building-a-Simple-Chatbot-in-Python-using-NLTK/blob/master/Chatbot.ipynb\n",
    "# https://stackoverflow.com/questions/15899861/efficient-term-document-matrix-with-nltk\n",
    "# https://github.com/williamscott701/Information-Retrieval/blob/master/2.%20TF-IDF%20Ranking%20-%20Cosine%20Similarity%2C%20Matching%20Score/TF-IDF.ipynb\n",
    "# Doc2Vec\n",
    "# https://medium.com/@mishra.thedeepak/doc2vec-simple-implementation-example-df2afbbfbad5\n",
    "# Word2Vec and Doc2Vec\n",
    "# https://shuzhanfan.github.io/2018/08/understanding-word2vec-and-doc2vec/\n",
    "# https://ireneli.eu/2016/07/27/nlp-05-from-word2vec-to-doc2vec-a-simple-example-with-gensim/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    data = []\n",
    "    f = open(\"Data/\" + filename, 'r', encoding=\"utf8\")\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        line = SentPreProcessing(line)\n",
    "        data.append(line)\n",
    "    return data\n",
    "\n",
    "\n",
    "def stemming(sent):\n",
    "    stemmer= PorterStemmer()\n",
    "    \n",
    "    tokens = word_tokenize(str(sent))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        new_text = new_text + \" \" + stemmer.stem(w)\n",
    "    return new_text\n",
    "\n",
    "def SentPreProcessing(sent):\n",
    "    word_tokens = word_tokenize(sent)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_sentence = [] \n",
    "    for w in word_tokens: \n",
    "        if w not in stop_words: \n",
    "            filtered_sentence.append(w) \n",
    "    sent = \" \".join(filtered_sentence)\n",
    "    sent = stemming(sent)\n",
    "    sent = sent.lower()\n",
    "    for i in string.punctuation:\n",
    "        sent = sent.replace(i, ' ')\n",
    "    sent = re.sub(r'[^\\w]', ' ', sent)\n",
    "    sent = re.sub(r'\\d+', '', sent)\n",
    "    return sent\n",
    "\n",
    "def getVocab(DocTokens, data):\n",
    "    Vocab = set()\n",
    "    for d in data:\n",
    "        tkns = word_tokenize(d)\n",
    "        DocTokens.append(tkns)\n",
    "        for t in tkns:\n",
    "            Vocab.add(t)\n",
    "    Vocab = list(Vocab)\n",
    "    return Vocab\n",
    "\n",
    "def Get_tfidf_Matrix(data, Vocab=None):\n",
    "    DocTokens = []\n",
    "    for d in data:\n",
    "        tkns = word_tokenize(d)\n",
    "        DocTokens.append(tkns)\n",
    "        \n",
    "    if Vocab == None:\n",
    "        Vocab = getVocab(DocTokens, data)\n",
    "        \n",
    "    tfidf = np.zeros((len(data), len(Vocab)))\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(Vocab)):\n",
    "            tfidf[i, j] = 1 + log(1 + DocTokens[i].count(Vocab[j]))\n",
    "            \n",
    "    N = len(data)\n",
    "    for i in range(len(Vocab)):\n",
    "        w = Vocab[i]\n",
    "        count = 0\n",
    "        for j in range(N):\n",
    "            if (w in DocTokens[j]):\n",
    "                count += 1\n",
    "        tfidf[:, i] = tfidf[:, i] * log(N/(count+1))\n",
    "        \n",
    "    return tfidf, Vocab\n",
    "\n",
    "def load_questions(filename):\n",
    "    data = []\n",
    "    f = open(\"Data/\" + filename, 'r')\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        data.append(json.loads(line))\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 4566)\n"
     ]
    }
   ],
   "source": [
    "Data = load_data(\"data.txt\")\n",
    "Questions = load_questions(\"test.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1  |  Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TermDocMat, Vocab = Get_tfidf_Matrix(Data)\n",
    "print(TermDocMat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 24.233333333333327\n"
     ]
    }
   ],
   "source": [
    "Alpha = [\"A\", \"B\", \"C\", \"D\"]\n",
    "QScores = []\n",
    "for ques in Questions:\n",
    "    Q = ques[\"question\"][\"stem\"]\n",
    "    A = ques[\"question\"][\"choices\"]\n",
    "    C = ques[\"answerKey\"]\n",
    "    Scores = []\n",
    "    for option in A:\n",
    "        tempQ = Q + \" \" + option[\"text\"]\n",
    "        tempQ = [SentPreProcessing(tempQ)]\n",
    "        temp_tfidf, _ = Get_tfidf_Matrix(tempQ, Vocab=Vocab)\n",
    "        Scores.append(np.max(cosine_similarity(temp_tfidf, TermDocMat)))\n",
    "    setOfOptions = []\n",
    "    maxi = max(Scores)\n",
    "    for i in range(len(Scores)):\n",
    "        if maxi == Scores[i]:\n",
    "            setOfOptions.append(Alpha[i])\n",
    "    if (C in setOfOptions):\n",
    "        QScores.append(1/len(setOfOptions))\n",
    "    else:\n",
    "        QScores.append(0)\n",
    "print(\"Accuracy:\", (sum(QScores)/len(Questions))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2     |     Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "TaggedData = []\n",
    "for i, _d in enumerate(Data):\n",
    "    tag_data = TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)])\n",
    "    TaggedData.append(tag_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 5\n",
    "vec_size = 20\n",
    "alpha = 0.025\n",
    "\n",
    "'''\n",
    "dm: If dm=1 means ‘distributed memory’ (PV-DM) and dm =0 means ‘distributed bag of words’ (PV-DBOW). \n",
    "Distributed Memory model preserves the word order in a document whereas Distributed Bag of words just uses the bag of words approach, \n",
    "which doesn’t preserve any word order.\n",
    "'''\n",
    "\n",
    "model = Doc2Vec(vector_size=vec_size, alpha=alpha, min_alpha=0.00025, min_count=1, dm=0)\n",
    "model.build_vocab(TaggedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kvats\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(max_epochs):\n",
    "    if epoch%10 == 0:\n",
    "        print('iteration {}'.format(epoch))\n",
    "    model.train(TaggedData, total_examples=model.corpus_count, epochs=model.iter)\n",
    "    model.alpha -= 0.0002\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "model.save(\"Data/Doc2Vec.model\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec.load(\"Data/Doc2Vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 26.0\n"
     ]
    }
   ],
   "source": [
    "Alpha = [\"A\", \"B\", \"C\", \"D\"]\n",
    "QScores = []\n",
    "for ques in Questions:\n",
    "    Q = ques[\"question\"][\"stem\"]\n",
    "    A = ques[\"question\"][\"choices\"]\n",
    "    C = ques[\"answerKey\"]\n",
    "    Scores = []\n",
    "    for option in A:\n",
    "        tempQ = Q + \" \" + option[\"text\"]\n",
    "        tempQ = SentPreProcessing(tempQ)\n",
    "        tempQ = word_tokenize(tempQ)\n",
    "        InferVec = model.infer_vector(tempQ)\n",
    "        SimilarDoc = model.docvecs.most_similar([InferVec])\n",
    "        Scores.append(SimilarDoc[1])\n",
    "        \n",
    "    setOfOptions = []\n",
    "    maxi = max(Scores)\n",
    "    for i in range(len(Scores)):\n",
    "        if maxi == Scores[i]:\n",
    "            setOfOptions.append(Alpha[i])\n",
    "    if (C in setOfOptions):\n",
    "        QScores.append(1/len(setOfOptions))\n",
    "    else:\n",
    "        QScores.append(0)\n",
    "print(\"Accuracy:\", (sum(QScores)/len(Questions))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
