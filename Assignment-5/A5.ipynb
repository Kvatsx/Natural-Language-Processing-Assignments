{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaustav Vats (2016048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, re\n",
    "import json\n",
    "from math import log\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "# References\n",
    "# Cosine Similarity and TFIDF Matrix\n",
    "# https://github.com/parulnith/Building-a-Simple-Chatbot-in-Python-using-NLTK/blob/master/Chatbot.ipynb\n",
    "# https://stackoverflow.com/questions/15899861/efficient-term-document-matrix-with-nltk\n",
    "# https://github.com/williamscott701/Information-Retrieval/blob/master/2.%20TF-IDF%20Ranking%20-%20Cosine%20Similarity%2C%20Matching%20Score/TF-IDF.ipynb\n",
    "# Doc2Vec\n",
    "# https://medium.com/@mishra.thedeepak/doc2vec-simple-implementation-example-df2afbbfbad5\n",
    "# Word2Vec and Doc2Vec\n",
    "# https://shuzhanfan.github.io/2018/08/understanding-word2vec-and-doc2vec/\n",
    "# https://ireneli.eu/2016/07/27/nlp-05-from-word2vec-to-doc2vec-a-simple-example-with-gensim/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    data = []\n",
    "    f = open(\"Data/\" + filename, 'r', encoding=\"utf8\")\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        line = SentPreProcessing(line)\n",
    "        data.append(line)\n",
    "    return data\n",
    "\n",
    "def stemming(sent):\n",
    "    stemmer= PorterStemmer()\n",
    "    \n",
    "    tokens = word_tokenize(str(sent))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        new_text = new_text + \" \" + stemmer.stem(w)\n",
    "    return new_text\n",
    "\n",
    "def SentPreProcessing(sent):\n",
    "    word_tokens = word_tokenize(sent)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_sentence = [] \n",
    "    for w in word_tokens: \n",
    "        if w not in stop_words: \n",
    "            filtered_sentence.append(w) \n",
    "    sent = \" \".join(filtered_sentence)\n",
    "    sent = stemming(sent)\n",
    "    sent = sent.lower()\n",
    "    for i in string.punctuation:\n",
    "        sent = sent.replace(i, ' ')\n",
    "    sent = re.sub(r'[^\\w]', ' ', sent)\n",
    "    sent = re.sub(r'\\d+', '', sent)\n",
    "    return sent\n",
    "\n",
    "def getVocab(DocTokens, data):\n",
    "    Vocab = set()\n",
    "    for d in data:\n",
    "        tkns = word_tokenize(d)\n",
    "        DocTokens.append(tkns)\n",
    "        for t in tkns:\n",
    "            Vocab.add(t)\n",
    "    Vocab = list(Vocab)\n",
    "    return Vocab\n",
    "\n",
    "def Get_tfidf_Matrix(data):\n",
    "    DocTokens = []\n",
    "    for d in data:\n",
    "        tkns = word_tokenize(d)\n",
    "        DocTokens.append(tkns)\n",
    "        \n",
    "    Vocab = getVocab(DocTokens, data)\n",
    "        \n",
    "    tfidf = np.zeros((len(data), len(Vocab)))\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(Vocab)):\n",
    "            tfidf[i, j] = 1 + log(1 + DocTokens[i].count(Vocab[j]))\n",
    "            \n",
    "    N = len(data)\n",
    "    IDF_Vector = []\n",
    "    for i in range(len(Vocab)):\n",
    "        w = Vocab[i]\n",
    "        count = 0\n",
    "        for j in range(N):\n",
    "            if (w in DocTokens[j]):\n",
    "                count += 1\n",
    "        IDF_Vector.append(log(N/(count+1)))\n",
    "        tfidf[:, i] = tfidf[:, i] * log(N/(count+1))\n",
    "        \n",
    "    return tfidf, Vocab, IDF_Vector\n",
    "\n",
    "def get_tfidf_query(data, vocab, idf_v):\n",
    "    DocTokens = []\n",
    "    for d in data:\n",
    "        tkns = word_tokenize(d)\n",
    "        DocTokens.append(tkns)\n",
    "    \n",
    "    tfidf = np.zeros((len(data), len(vocab)))\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(vocab)):\n",
    "            tfidf[i, j] = 1 + log(1 + DocTokens[i].count(vocab[j]))\n",
    "    \n",
    "    for i in range(len(vocab)):\n",
    "        tfidf[:, i] = tfidf[:, i] * idf_v[i]\n",
    "        \n",
    "    return tfidf\n",
    "\n",
    "def load_questions(filename):\n",
    "    data = []\n",
    "    f = open(\"Data/\" + filename, 'r')\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "def ShowAnalysis(Analysis, count):\n",
    "    for i in range(count):\n",
    "        print(Analysis[i])\n",
    "\n",
    "def FindMax(S):\n",
    "    maxi = S[0][1]\n",
    "    for se in S:\n",
    "        if (maxi < se[1]):\n",
    "            maxi = se[1]\n",
    "    return maxi\n",
    "def Counts(Analysis):\n",
    "    ABCD = [0, 0, 0, 0]\n",
    "    for i in range(len(Analysis)):\n",
    "        for sym in Analysis[i][2]:\n",
    "            if (Analysis[i][1] != sym):\n",
    "                if sym == 'A':\n",
    "                    ABCD[0] += 1\n",
    "                elif sym == 'B':\n",
    "                    ABCD[1] += 1\n",
    "                elif sym == 'C':\n",
    "                    ABCD[2] += 1\n",
    "                else:\n",
    "                    ABCD[3] += 1\n",
    "    return ABCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Data = load_data(\"data.txt\")\n",
    "Questions = load_questions(\"test.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1  |  Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 4566)\n"
     ]
    }
   ],
   "source": [
    "TermDocMat, Vocab, IDF_V = Get_tfidf_Matrix(Data)\n",
    "print(TermDocMat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94852932b5c1479aaf96d1f9b3b2404c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 22.56666666666666\n"
     ]
    }
   ],
   "source": [
    "Alpha = [\"A\", \"B\", \"C\", \"D\"]\n",
    "QScores = []\n",
    "for ques in tqdm(Questions):\n",
    "    Q = ques[\"question\"][\"stem\"]\n",
    "    A = ques[\"question\"][\"choices\"]\n",
    "    C = ques[\"answerKey\"]\n",
    "    Scores = []\n",
    "    for option in A:\n",
    "        tempQ = Q + \" \" + option[\"text\"]\n",
    "        tempQ = [SentPreProcessing(tempQ)]\n",
    "        temp_tfidf = get_tfidf_query(tempQ, Vocab, IDF_V)\n",
    "        Scores.append(np.amax(cosine_similarity(temp_tfidf, TermDocMat)))\n",
    "    setOfOptions = []\n",
    "    maxi = max(Scores)\n",
    "    for i in range(len(Scores)):\n",
    "        if maxi == Scores[i]:\n",
    "            setOfOptions.append(Alpha[i])\n",
    "    if (C in setOfOptions):\n",
    "        QScores.append(1/len(setOfOptions))\n",
    "    else:\n",
    "        QScores.append(0)\n",
    "print(\"Accuracy:\", (sum(QScores)/len(Questions))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2     |     Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "TaggedData = []\n",
    "for i, _d in enumerate(Data):\n",
    "    tag_data = TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)])\n",
    "    TaggedData.append(tag_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 50\n",
    "vec_size = 20\n",
    "alpha = 0.025\n",
    "\n",
    "'''\n",
    "dm: If dm=1 means ‘distributed memory’ (PV-DM) and dm =0 means ‘distributed bag of words’ (PV-DBOW). \n",
    "Distributed Memory model preserves the word order in a document whereas Distributed Bag of words just uses the bag of words approach, \n",
    "which doesn’t preserve any word order.\n",
    "'''\n",
    "\n",
    "model = Doc2Vec(vector_size=vec_size, alpha=alpha, min_alpha=0.00025, min_count=1, dm=0)\n",
    "model.build_vocab(TaggedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kvats\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 10\n",
      "iteration 20\n",
      "iteration 30\n",
      "iteration 40\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(max_epochs):\n",
    "    if epoch%10 == 0:\n",
    "        print('iteration {}'.format(epoch))\n",
    "    model.train(TaggedData, total_examples=model.corpus_count, epochs=model.iter)\n",
    "    model.alpha -= 0.0002\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "model.save(\"Data/Doc2Vec.model\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec.load(\"Data/Doc2Vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 21.0\n"
     ]
    }
   ],
   "source": [
    "Alpha = [\"A\", \"B\", \"C\", \"D\"]\n",
    "QScores = []\n",
    "Analysis = []\n",
    "for ques in Questions:\n",
    "    Q = ques[\"question\"][\"stem\"]\n",
    "    A = ques[\"question\"][\"choices\"]\n",
    "    C = ques[\"answerKey\"]\n",
    "    Scores = []\n",
    "    for option in A:\n",
    "        tempQ = Q + \" \" + option[\"text\"]\n",
    "        tempQ = SentPreProcessing(tempQ)\n",
    "        tempQ = word_tokenize(tempQ)\n",
    "        InferVec = model.infer_vector(tempQ)\n",
    "        SimilarDoc = model.docvecs.most_similar([InferVec])\n",
    "        maxi = FindMax(SimilarDoc)\n",
    "        Scores.append(maxi)\n",
    "    setOfOptions = []\n",
    "    maxi = max(Scores)\n",
    "    for i in range(len(Scores)):\n",
    "        if maxi == Scores[i]:\n",
    "            setOfOptions.append(Alpha[i])\n",
    "    if (C in setOfOptions):\n",
    "        QScores.append(1/len(setOfOptions))\n",
    "    else:\n",
    "        QScores.append(0)\n",
    "    Analysis.append((maxi, C, setOfOptions))\n",
    "    \n",
    "print(\"Accuracy:\", (sum(QScores)/len(Questions))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.847078800201416, 'A', ['D'])\n",
      "(0.8590711355209351, 'B', ['B'])\n",
      "(0.9049407243728638, 'D', ['D'])\n",
      "(0.8289058208465576, 'D', ['A'])\n",
      "(0.8626338243484497, 'B', ['B'])\n",
      "(0.8186124563217163, 'C', ['B'])\n",
      "(0.8212941884994507, 'A', ['A'])\n",
      "(0.8327691555023193, 'C', ['A'])\n",
      "(0.8815698027610779, 'C', ['D'])\n",
      "(0.838552713394165, 'A', ['C'])\n",
      "(0.8444642424583435, 'B', ['D'])\n",
      "(0.8332182168960571, 'B', ['C'])\n",
      "(0.8259787559509277, 'B', ['C'])\n",
      "(0.8639709949493408, 'B', ['B'])\n",
      "(0.8211303949356079, '2', ['B'])\n",
      "(0.8195202350616455, 'B', ['A'])\n",
      "(0.7896223068237305, 'B', ['D'])\n",
      "(0.8315780162811279, 'D', ['C'])\n",
      "(0.8351916670799255, 'B', ['C'])\n",
      "(0.8623121380805969, 'D', ['C'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[109, 90, 80, 116]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ShowAnalysis(Analysis, 20)\n",
    "Counts(Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
