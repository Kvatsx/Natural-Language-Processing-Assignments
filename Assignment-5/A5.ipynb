{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaustav Vats (2016048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, re\n",
    "import json\n",
    "from math import log\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "# https://medium.com/@mishra.thedeepak/doc2vec-simple-implementation-example-df2afbbfbad5\n",
    "# https://github.com/parulnith/Building-a-Simple-Chatbot-in-Python-using-NLTK/blob/master/Chatbot.ipynb\n",
    "# https://stackoverflow.com/questions/15899861/efficient-term-document-matrix-with-nltk\n",
    "# https://github.com/williamscott701/Information-Retrieval/blob/master/2.%20TF-IDF%20Ranking%20-%20Cosine%20Similarity%2C%20Matching%20Score/TF-IDF.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    data = []\n",
    "    f = open(\"Data/\" + filename, 'r', encoding=\"utf8\")\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        line = SentPreProcessing(line)\n",
    "        data.append(line)\n",
    "    return data\n",
    "\n",
    "\n",
    "def stemming(sent):\n",
    "    stemmer= PorterStemmer()\n",
    "    \n",
    "    tokens = word_tokenize(str(sent))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        new_text = new_text + \" \" + stemmer.stem(w)\n",
    "    return new_text\n",
    "\n",
    "def SentPreProcessing(sent):\n",
    "    word_tokens = word_tokenize(sent)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_sentence = [] \n",
    "    for w in word_tokens: \n",
    "        if w not in stop_words: \n",
    "            filtered_sentence.append(w) \n",
    "    sent = \" \".join(filtered_sentence)\n",
    "    sent = stemming(sent)\n",
    "    sent = sent.lower()\n",
    "    for i in string.punctuation:\n",
    "        sent = sent.replace(i, ' ')\n",
    "    sent = re.sub(r'[^\\w]', ' ', sent)\n",
    "    sent = re.sub(r'\\d+', '', sent)\n",
    "    return sent\n",
    "\n",
    "def getVocab(DocTokens, data):\n",
    "    Vocab = set()\n",
    "    for d in data:\n",
    "        tkns = word_tokenize(d)\n",
    "        DocTokens.append(tkns)\n",
    "        for t in tkns:\n",
    "            Vocab.add(t)\n",
    "    Vocab = list(Vocab)\n",
    "    return Vocab\n",
    "\n",
    "def Get_tfidf_Matrix(data, Vocab=None):\n",
    "    DocTokens = []\n",
    "    for d in data:\n",
    "        tkns = word_tokenize(d)\n",
    "        DocTokens.append(tkns)\n",
    "        \n",
    "    if Vocab == None:\n",
    "        Vocab = getVocab(DocTokens, data)\n",
    "        \n",
    "    tfidf = np.zeros((len(data), len(Vocab)))\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(Vocab)):\n",
    "            tfidf[i, j] = 1 + log(1 + DocTokens[i].count(Vocab[j]))\n",
    "            \n",
    "    N = len(data)\n",
    "    for i in range(len(Vocab)):\n",
    "        w = Vocab[i]\n",
    "        count = 0\n",
    "        for j in range(N):\n",
    "            if (w in DocTokens[j]):\n",
    "                count += 1\n",
    "        tfidf[:, i] = tfidf[:, i] * log(N/(count+1))\n",
    "        \n",
    "    return tfidf, Vocab\n",
    "\n",
    "def load_questions(filename):\n",
    "    data = []\n",
    "    f = open(\"Data/\" + filename, 'r')\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        data.append(json.loads(line))\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 4566)\n"
     ]
    }
   ],
   "source": [
    "Data = load_data(\"data.txt\")\n",
    "TermDocMat, Vocab = Get_tfidf_Matrix(Data)\n",
    "print(TermDocMat.shape)\n",
    "Questions = load_questions(\"test.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.24266666666666661\n"
     ]
    }
   ],
   "source": [
    "Alpha = [\"A\", \"B\", \"C\", \"D\"]\n",
    "QScores = []\n",
    "for ques in Questions:\n",
    "    Q = ques[\"question\"][\"stem\"]\n",
    "    A = ques[\"question\"][\"choices\"]\n",
    "    C = ques[\"answerKey\"]\n",
    "    Scores = []\n",
    "    for option in A:\n",
    "        tempQ = Q + \" \" + option[\"text\"]\n",
    "        tempQ = [SentPreProcessing(tempQ)]\n",
    "        temp_tfidf, _ = Get_tfidf_Matrix(tempQ, Vocab=Vocab)\n",
    "        Scores.append(np.max(cosine_similarity(temp_tfidf, TermDocMat)))\n",
    "    setOfOptions = []\n",
    "    maxi = max(Scores)\n",
    "    for i in range(len(Scores)):\n",
    "        if maxi == Scores[i]:\n",
    "            setOfOptions.append(Alpha[i])\n",
    "    if (C in setOfOptions):\n",
    "        QScores.append(1/len(setOfOptions))\n",
    "    else:\n",
    "        QScores.append(0)\n",
    "print(\"Accuracy:\", sum(QScores)/len(Questions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2     |     Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
