{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaustav Vats (2016048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import json\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Grammar, Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Grammer\n",
    "Grammar = nltk.data.load(\"grammars/large_grammars/atis.cfg\")\n",
    "# Grammar = nltk.CFG.fromstring(\"\"\"\n",
    "# A -> B C D\n",
    "# B -> E F | G\n",
    "# C -> 'c'\n",
    "# D -> C B G\n",
    "# E -> F G\n",
    "# E -> 'e'\n",
    "# F -> 'f'\n",
    "# G -> 'g'\n",
    "# \"\"\")\n",
    "old_productions = Grammar.productions()\n",
    "\n",
    "# Load Raw Sentences\n",
    "Sentences = nltk.data.load(\"grammars/large_grammars/atis_sentences.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All required functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, root, left, right, val):\n",
    "        self.root = root\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.val = val\n",
    "        \n",
    "    def __str__(self):\n",
    "        return self.root\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "def save(dic):\n",
    "    with open('productions.json', 'w') as fp:\n",
    "        json.dump(dic, fp)\n",
    "\n",
    "def pre_process_grammar(prod):\n",
    "    nprod = {}\n",
    "    for i in range(len(prod)):\n",
    "        k = str(prod[i].lhs())\n",
    "        v = list(prod[i].rhs())\n",
    "#         print(v)\n",
    "        \n",
    "        for i in range(len(v)):\n",
    "            if isinstance(v[i], str) or isinstance(v[i], bytes):\n",
    "                v[i] = \"'\" + v[i] + \"'\"\n",
    "            else:\n",
    "                v[i] = str(v[i])\n",
    "        \n",
    "        if k not in nprod:\n",
    "            nprod[k] = [v]\n",
    "        else:\n",
    "            if v not in nprod[k]:\n",
    "                 nprod[k].append(v)\n",
    "    save(nprod)\n",
    "    return nprod\n",
    "\n",
    "def match(dic, label, x, y):\n",
    "    for i in range(len(dic)):\n",
    "        if dic[i][0] == x and dic[i][1] == y:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "def RemoveLargeRules(old_prod):\n",
    "    count = 0\n",
    "    dic = []\n",
    "    label = []\n",
    "    temp_prod = copy.deepcopy(old_prod)\n",
    "    for k in temp_prod:\n",
    "        v = temp_prod[k]\n",
    "        for idx in range(len(v)):\n",
    "            if (len(v[idx]) > 2):  # rule with more than 2 non terminal\n",
    "                prev = v[idx][0]\n",
    "                for i in range(1, len(v[idx])-1):\n",
    "                    curr = v[idx][i]\n",
    "                    index = match(dic, label, prev, curr)\n",
    "                    if (index == -1):\n",
    "                        letter = \"NR\" + str(count)\n",
    "                        count += 1\n",
    "                        old_prod[letter] = [[prev, curr]]\n",
    "                        dic.append([prev, curr])\n",
    "                        label.append(letter)\n",
    "                        prev = letter\n",
    "                    else:\n",
    "                        letter = label[index]\n",
    "                        prev = letter\n",
    "                old_prod[k][idx] = [prev, v[idx][len(v[idx])-1]]\n",
    "                        \n",
    "    return old_prod\n",
    "    \n",
    "def RemoveUnitRules(old_prod):\n",
    "    ustate = list(old_prod.keys())\n",
    "    flag = True\n",
    "    while flag:\n",
    "        flag = False\n",
    "        temp_productions = copy.deepcopy(old_prod)\n",
    "        for k in temp_productions:\n",
    "            v = temp_productions[k]\n",
    "            for idx in range(len(v)):\n",
    "                if (len(v[idx]) == 1 and v[idx][0] in ustate and v[idx][0] != k):\n",
    "                    flag = True\n",
    "                    k2 = copy.deepcopy(v[idx][0])\n",
    "                    v2 = old_prod[k2]\n",
    "                    old_prod[k].remove(v[idx])\n",
    "                    for j in range(len(v2)):\n",
    "                        old_prod[k].append(v2[j])\n",
    "    return old_prod\n",
    "\n",
    "def RemoveDuplicate(old_prod):\n",
    "    temp_prod = copy.deepcopy(old_prod)\n",
    "    for k in temp_prod:\n",
    "        v = temp_prod[k]\n",
    "        new_v = []\n",
    "        for e in v:\n",
    "            if e not in new_v:\n",
    "                new_v.append(e)\n",
    "            else:\n",
    "                old_prod[k].remove(e)\n",
    "    return old_prod\n",
    "\n",
    "def save_CNF(old_prod):\n",
    "    with open(\"CNF.txt\", 'w') as fp:\n",
    "        for k in old_prod:\n",
    "            v = old_prod[k]\n",
    "            res = \"\"\n",
    "            for idx in range(len(v)):\n",
    "                res = \"\"\n",
    "                for i in range(len(v[idx])):\n",
    "                    res += str(v[idx][i]) + \" \"\n",
    "                fp.write(str(k) + \" -> \" + res + \"\\n\")\n",
    "        \n",
    "    \n",
    "\"\"\"\n",
    "Assumptions and Observations\n",
    "1. Eliminate start symbol from RHS. = This case is not present in given grammar, no such grammar would be given which has this case.\n",
    "2. Eliminate null = This case is not present in given grammar, no such grammar would be given which has this case.\n",
    "3. Eliminate terminals from RHS if they exist with other terminals or non-terminals = This case is not present in given grammar, no such grammar would be given which has this case.\n",
    "4. \n",
    "\"\"\"\n",
    "def CFG2CNF(old_prod):\n",
    "    old_prod = pre_process_grammar(old_prod)\n",
    "    old_prod = RemoveUnitRules(old_prod)\n",
    "    \n",
    "#     old_prod = large(old_prod)\n",
    "    old_prod = RemoveLargeRules(old_prod)\n",
    "    old_prod = RemoveDuplicate(old_prod)\n",
    "    save_CNF(old_prod)\n",
    "    return old_prod\n",
    "\n",
    "def PreProcess(old_prod):\n",
    "    old_prod = pre_process_grammar(old_prod)\n",
    "    save_CNF(old_prod)\n",
    "    return old_prod\n",
    "    \n",
    "def cky_parser(sent, old_prod):\n",
    "    sentence = []\n",
    "    for i in range(len(sent)):\n",
    "        sentence.append(\"'\" + sent[i] + \"'\")\n",
    "    DP = []\n",
    "    NodeMat = []\n",
    "    for i in range(len(sentence)):\n",
    "        DP.append([])\n",
    "        NodeMat.append([])\n",
    "        for j in range(len(sentence)):\n",
    "            DP[i].append([])\n",
    "            NodeMat[i].append([])\n",
    "            \n",
    "    # Bottom Up approach to fill Mat\n",
    "    for i in range(1, len(sentence)):\n",
    "        for k in old_prod:\n",
    "            v = old_prod[k]\n",
    "            for rhs in v:\n",
    "                if (len(rhs) == 1 and rhs[0] == sentence[i-1]):\n",
    "                    DP[i-1][i].append(k)\n",
    "                    NodeMat[i-1][i].append(Node(k, None, None, sentence[i-1]))\n",
    "                    \n",
    "        for j in range(i-1, -1, -1):\n",
    "            for k in range(j+1, i):\n",
    "                for key in old_prod:\n",
    "                    v = old_prod[key]\n",
    "                    for idx in range(len(v)):\n",
    "                        if (len(v[idx]) == 2):  # A = BC\n",
    "                            B = v[idx][0]\n",
    "                            C = v[idx][1]\n",
    "                            if B in DP[j][k] and C in DP[k][i]:\n",
    "                                DP[j][i].append(key)\n",
    "                                for b in NodeMat[j][k]:\n",
    "                                    for c in NodeMat[k][i]:\n",
    "                                        if b.root == B and c.root == C:\n",
    "                                            NodeMat[j][i].append(Node(key, b, c, None))\n",
    "                                            \n",
    "    return NodeMat[0][len(sentence)-1], NodeMat\n",
    "\n",
    "# def CKY_Parser(sent, old_prod):\n",
    "#     n = len(sent)\n",
    "#     sentence = []\n",
    "#     for i in range(n):\n",
    "#         sentence.append(\"'\" + sent[i] + \"'\")\n",
    "#     # Initialization step\n",
    "#     Mat = []\n",
    "#     DP = []\n",
    "#     for i in range(n):\n",
    "#         Mat.append([])\n",
    "#         DP.append([])\n",
    "#         for j in range(n):\n",
    "#             Mat[i].append([])\n",
    "#             DP[i].append([])\n",
    "    \n",
    "#     # For Variables with 1 substring\n",
    "#     for i in range(n):\n",
    "#         for key in old_prod:\n",
    "#             val = old_prod[key]\n",
    "#             for rhs in val:\n",
    "#                 if (len(rhs) == 1 and rhs[0] == sentence[i]):\n",
    "#                     Mat[i-1][i].append(key)\n",
    "#                     DP[i-1][i].append(Node(key, None, None, sentence[i]))\n",
    "                    \n",
    "#     for i in range(len(Mat)):\n",
    "#         print()\n",
    "#         for j in range(len(Mat[i])):\n",
    "#             print(Mat[i][j], end=' ')\n",
    "    \n",
    "#     # For rest of the variables\n",
    "#     for j in range(2, n):\n",
    "#         for i in range(j-2, -1, -1):\n",
    "#             for k in range(i+1, j-2):\n",
    "#                 for key in old_prod:\n",
    "#                     val = old_prod[key]\n",
    "#                     for idx in range(len(val)):\n",
    "#                         if len(val[idx]) == 2:  # A = BC\n",
    "#                             B = val[idx][0]\n",
    "#                             C = val[idx][1]\n",
    "#                             if B in Mat[i][k] and C in Mat[k][j]:\n",
    "#                                 Mat[i][j].append(key)\n",
    "#                                 for b in DP[i][k]:\n",
    "#                                     for c in DP[k][j]:\n",
    "#                                         if b.root == B and c.root == C:\n",
    "#                                             DP[i][j].append(Node(key, b, c, None))\n",
    "\n",
    "#     return DP[0][n-1], DP\n",
    "\n",
    "def ShowTree(root):\n",
    "    if root.val != None:\n",
    "        return \"(\" + root.root + \" \" + root.val + \")\"\n",
    "    return \"(\" + root.root + \" \" + ShowTree(root.left) + \" \" +  ShowTree(root.right) + \")\"\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This step converts production rules to CNF form with some preconditions and assumptions, mentioned with the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old_prod = pre_process_grammar(old_productions)\n",
    "# old_prod = CFG2CNF(old_productions)\n",
    "G = Grammar.chomsky_normal_form(new_token_padding=\"_\")\n",
    "old_productions = G.productions()\n",
    "old_prod = PreProcess(old_productions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Test Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = nltk.data.load(\"grammars/large_grammars/atis_sentences.txt\")\n",
    "t = nltk.parse.util.extract_test_sentences(s)\n",
    "sentences = []\n",
    "for sent in t:\n",
    "    sentences.append(sent[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part - 1  |  Below prints the CKY parser count for all test sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2085 0\n",
      "1380 402\n",
      "50 13\n",
      "18 0\n",
      "0 0\n",
      "20 0\n",
      "0 0\n",
      "0 0\n",
      "1059 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "54 0\n",
      "3 0\n",
      "55 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "3 2\n",
      "17 0\n",
      "2 0\n",
      "2 1\n",
      "11 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "597 202\n",
      "44 0\n",
      "0 0\n",
      "437 3\n",
      "1 0\n",
      "6 0\n",
      "15 15\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "598 0\n",
      "8913 1690\n",
      "569 0\n",
      "28250 10154\n",
      "9 0\n",
      "1010 0\n",
      "6153 0\n",
      "32 0\n",
      "7 0\n",
      "8 0\n",
      "136 0\n",
      "295 0\n",
      "21 0\n",
      "10 2\n",
      "5 1\n",
      "3 2\n",
      "10 0\n",
      "3 2\n",
      "0 0\n",
      "10 0\n",
      "36122 0\n",
      "6 1\n",
      "9 0\n",
      "293 275\n",
      "0 0\n",
      "0 0\n",
      "2 1\n",
      "0 0\n",
      "5 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "13 0\n",
      "0 0\n",
      "716 321\n",
      "0 0\n",
      "22 8\n",
      "0 0\n",
      "0 0\n",
      "5 1\n",
      "19 17\n",
      "2 2\n",
      "2 2\n",
      "11 7\n",
      "5 2\n",
      "24 12\n",
      "0 5\n",
      "200 87\n",
      "200 87\n",
      "72 0\n",
      "4 2\n",
      "354 0\n",
      "229 3\n",
      "46 18\n",
      "106 0\n",
      "85 0\n",
      "17 4\n",
      "1645 0\n",
      "7 0\n"
     ]
    }
   ],
   "source": [
    "Ans = []\n",
    "start = str(Grammar.start())\n",
    "for i in range(len(sentences)):\n",
    "# i = 2\n",
    "    bt, DP = cky_parser(sentences[i], old_prod) \n",
    "    ntree = 0\n",
    "\n",
    "    #     for i in range(len(DP)):\n",
    "    #         print()\n",
    "    #         for j in range(len(DP[i])):\n",
    "    #             print(DP[i][j], end=' ')\n",
    "    for node in bt:\n",
    "        if node.root == start:\n",
    "            ntree += 1\n",
    "    Ans.append(str(ntree))\n",
    "\n",
    "    # OriginalParse Count and My parser parse Count\n",
    "    print(str(t[i][1]) + \" \" + str(ntree))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2   |   Print Tree and Draw them using nltk.draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(SIGMA (AVP_RB 'please') (NP_NNS_AVP_RB (NP_NN (NP_NN 'list') (NP_NN_NP_NN (NP_NN (NP_NNS (NOUN_NNS 'costs') (PREP_IN 'of')) (NOUN_NN 'round')) (NOUN_NN 'trip'))) (NP_NNS_AVP_RB_NP_NN (NOUN_NNS 'fares') (NP_NNS_AVP_RB_NP_NN_NOUN_NNS (PP_NP (PREP_IN 'from') (NOUN_NP 'denver')) (PP_NP (PREP_IN 'to') (NOUN_NP 'atlanta'))))))\n",
      "\n",
      "\n",
      "(SIGMA (AVP_RB 'please') (NP_NNS_AVP_RB (NP_NN (NOUN_NN 'list') (AVPNP_NN (NP_NN (NP_NNS (NOUN_NNS 'costs') (PREP_IN 'of')) (NOUN_NN 'round')) (NOUN_NN 'trip'))) (NP_NNS_AVP_RB_NP_NN (NOUN_NNS 'fares') (NP_NNS_AVP_RB_NP_NN_NOUN_NNS (PP_NP (PREP_IN 'from') (NOUN_NP 'denver')) (PP_NP (PREP_IN 'to') (NOUN_NP 'atlanta'))))))\n",
      "\n",
      "\n",
      "(SIGMA (AVP_RB 'please') (NP_NNS_AVP_RB (NP_NN (NP_NNS (NP_NN 'list') (NP_NNS_NP_NN (NOUN_NNS 'costs') (PP_NN (PREP_IN 'of') (NOUN_NN 'round')))) (NOUN_NN 'trip')) (NP_NNS_AVP_RB_NP_NN (NOUN_NNS 'fares') (NP_NNS_AVP_RB_NP_NN_NOUN_NNS (PP_NP (PREP_IN 'from') (NOUN_NP 'denver')) (PP_NP (PREP_IN 'to') (NOUN_NP 'atlanta'))))))\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for i in range(len(Ans)):\n",
    "#     print(i, Ans[i])\n",
    "bt, DP = cky_parser(sentences[91], old_prod) \n",
    "Trees = []\n",
    "for node in bt:\n",
    "    if node.root == start:\n",
    "        ntree += 1\n",
    "        tr = ShowTree(node)\n",
    "        Trees.append(tr)\n",
    "        print(tr)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to draw trees\n",
    "for tr in Trees:\n",
    "    tree = nltk.Tree.fromstring(tr)\n",
    "    tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['please', 'list', 'costs', 'of', 'round', 'trip', 'fares', 'from', 'denver', 'to', 'atlanta', '.']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[91])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
