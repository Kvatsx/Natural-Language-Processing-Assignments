{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment - 2\n",
    "### Kaustav Vats (2016048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import string\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import re, os\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "from random import randint as randi\n",
    "from math import log10\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetAllTokens(fileNameList, FolderPath):\n",
    "    tokens = []\n",
    "    for fileName in fileNameList:\n",
    "        file = open(FolderPath + fileName, 'r')\n",
    "        text = pre_processing(file.read())\n",
    "        tokens += word_tokenize(text)\n",
    "        file.close()\n",
    "    return tokens\n",
    "\n",
    "def GetTokens(folderName):\n",
    "    folderPath = \"20_newsgroups/\" + folderName + \"/\"\n",
    "    All_folders = os.listdir(folderPath)\n",
    "    TestFolderCount = int(len(All_folders) * 0.3)\n",
    "    Test_Tokens = []\n",
    "    Train_Tokens = []\n",
    "\n",
    "    for i in range(TestFolderCount):\n",
    "        index = randi(0, len(All_folders))\n",
    "        doc = All_folders.pop(index)\n",
    "        file = open(folderPath + doc, 'r')\n",
    "        text = pre_processing(file.read())\n",
    "        temp = word_tokenize(text)\n",
    "        Test_Tokens += temp\n",
    "        file.close()\n",
    "        \n",
    "    for doc in All_folders:\n",
    "        file = open(folderPath + doc, 'r')\n",
    "        text = pre_processing(file.read())\n",
    "        temp = word_tokenize(text)\n",
    "        Train_Tokens += temp\n",
    "        file.close()\n",
    "        \n",
    "    return Train_Tokens, Test_Tokens, \n",
    "    \n",
    "def pre_processing(text):\n",
    "    header = text.find(\"\\n\\n\")\n",
    "    text = text[header:]\n",
    "    text = text.strip()\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w]', ' ', text) # Word Preprocessing\n",
    "    return text\n",
    "\n",
    "def BagOfWords(tokens):\n",
    "    bof = {}\n",
    "    for t in tokens:\n",
    "        if (t in bof):\n",
    "            bof[t] += 1\n",
    "        else:\n",
    "            bof[t] = 1\n",
    "    return bof\n",
    "\n",
    "def TrainTestSplit(folderName):\n",
    "    folderPath = \"20_newsgroups/\" + folderName + \"/\"\n",
    "    All_folders = os.listdir(folderPath)\n",
    "    X1, X2 = train_test_split(All_folders, test_size=0.3)\n",
    "    return X1, X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14709\n",
      "13204\n",
      "22401\n"
     ]
    }
   ],
   "source": [
    "# Main Section\n",
    "# 2 Class\n",
    "\n",
    "# Folder Split\n",
    "mTrain, mTest = TrainTestSplit(\"rec.motorcycles\")\n",
    "bTrain, bTest = TrainTestSplit(\"rec.sport.baseball\")\n",
    "\n",
    "\n",
    "# For Training on Full Dataset\n",
    "mTrain = mTrain + mTest\n",
    "bTrain = bTrain + bTest\n",
    "\n",
    "mTokens = GetAllTokens(mTrain, \"20_newsgroups/rec.motorcycles/\")\n",
    "bTokens = GetAllTokens(bTrain, \"20_newsgroups/rec.sport.baseball/\")\n",
    "\n",
    "bow_m = BagOfWords(mTokens)\n",
    "bow_b = BagOfWords(bTokens)\n",
    "Vocab = list(set(list(bow_m.keys()) + list(bow_b.keys())))\n",
    "VocabDic = {}\n",
    "for i in range(len(Vocab)):\n",
    "    VocabDic[Vocab[i]] = i\n",
    "print(len(bow_m))\n",
    "print(len(bow_b))\n",
    "print(len(Vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5, 0.5]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def Fit(K=1):\n",
    "    if (os.path.isfile(\"Likli_Proba_2_{}\".format(K))):\n",
    "        LiklihoodProba = np.load(\"Likli_Proba_2_{}\".format(K))\n",
    "        return LiklihoodProba\n",
    "    LiklihoodProba = np.zeros((len(Vocab), 2))\n",
    "    for i in range(len(Vocab)):\n",
    "        word = Vocab[i]\n",
    "        if(word in bow_m):\n",
    "            wc = bow_m[word] + K\n",
    "            den = len(mTokens) + K*len(Vocab)\n",
    "            LiklihoodProba[i, 0] = wc/den\n",
    "        else:\n",
    "            wc = K\n",
    "            den = len(mTokens) + K*len(Vocab)\n",
    "            LiklihoodProba[i, 0] = wc/den\n",
    "        if(word in bow_b):\n",
    "            wc = bow_b[word] + K\n",
    "            den = len(bTokens) + K*len(Vocab)\n",
    "            LiklihoodProba[i, 1] = wc/den\n",
    "        else:\n",
    "            wc = K\n",
    "            den = len(bTokens) + K*len(Vocab)\n",
    "            LiklihoodProba[i, 1] = wc/den\n",
    "    np.save(\"Likli_Proba_2_{}\".format(K), LiklihoodProba)\n",
    "    return LiklihoodProba\n",
    "            \n",
    "            \n",
    "Priors = [len(mTrain)/(len(mTrain)+len(bTrain)), len(bTrain)/(len(mTrain)+len(bTrain))]\n",
    "print(Priors)\n",
    "def Predict(Doc, LiklihoodProba, K=1):\n",
    "    file = open(Doc, 'r')\n",
    "    text = pre_processing(file.read())\n",
    "    tokens = word_tokenize(text)\n",
    "    file.close()\n",
    "    \n",
    "    probs = [log10(Priors[0]), log10(Priors[1])]\n",
    "    # Class 0 & 1\n",
    "    for i in range(len(tokens)):\n",
    "        if (tokens[i] in Vocab):\n",
    "            probs[0] += log10(LiklihoodProba[VocabDic[tokens[i]], 0])\n",
    "            probs[1] += log10(LiklihoodProba[VocabDic[tokens[i]], 1])\n",
    "        else:\n",
    "            probs[0] += log10(K/(len(mTokens) + K*len(Vocab)))\n",
    "            probs[1] += log10(K/(len(bTokens) + K*len(Vocab)))\n",
    "    if (probs[0] > probs[1]):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in [1, 5, 10, 100]:\n",
    "    LiklihoodProba = Fit(K=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2e6457aa3f949aaa5321e77d7e28c1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=300), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "300\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "553385a0e0874bc7a945a28fa337caa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=300), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy[K=1]:  0.9983333333333333\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e079e214a06e4973ad3798775d5cab2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=300), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "299\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e846062567d47c891bbe324e23841e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=300), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy[K=5]:  0.995\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6089ec53cf9b40a3aac671b2f7a0cac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=300), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "295\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d9c96ef11c445c6809db3962301e9c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=300), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy[K=10]:  0.9883333333333333\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4185e02b2ab45948b6603e4e040954a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=300), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "241\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "371b88cf3ec4441c8f167b722e9d6320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=300), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy[K=100]:  0.8983333333333333\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 5, 10, 100]:\n",
    "    LiklihoodProba = Fit(K=k)\n",
    "    correct = 0\n",
    "    for i in tqdm(range(len(mTest))):\n",
    "    #     print(mTest[i])\n",
    "        c = Predict(\"20_newsgroups/rec.motorcycles/\" + mTest[i], LiklihoodProba, K=k)\n",
    "        if (c == 0):\n",
    "            correct+=1\n",
    "    print(correct)\n",
    "    for i in tqdm(range(len(bTest))):\n",
    "        c = Predict(\"20_newsgroups/rec.sport.baseball/\"+bTest[i], LiklihoodProba, K=k)\n",
    "        if (c == 1):\n",
    "            correct+=1\n",
    "    print(\"Accuracy[K={}]: \".format(k), correct/(len(mTest)+len(bTest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = str(input())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
